Se crea un fichero twitter.avsc (schema) y partiendo del fichero twitter.json (json) se construye el fichero twitter.avro (avro) desde java con la sentencia:

java -jar avro-tools-1.7.6.jar fromjson --schema-file twitter.avsc twitter.json > twitter.avro


Se suben a hdfs tanto el .avro como el .avsc (previamente se crean los directorios):

hadoop fs -copyFromLocal twitter.avro /user/cloudera/twitter/avro
hadoop fs -copyFromLocal twitter.avsc /user/cloudera/twitter/schema


Se crea la table en Hive con el schema avro:

CREATE EXTERNAL TABLE tweets
    COMMENT "A table backed by Avro data with the Avro schema stored in HDFS"
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
    STORED AS
    INPUTFORMAT  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
    LOCATION '/user/cloudera/twitter/avro'
    TBLPROPERTIES (
        'avro.schema.url'='hdfs:///user/cloudera/twitter/schema/twitter.avsc'
    );

Se consulta el resultado de forma satisfactoria:

SELECT * FROM tweets;


Se crean tablas nuevas a traves de subconsultas de forma satisfactoria:

CREATE TABLE tweets1 AS SELECT username,tweet,count(1) as CUENTA FROM tweets GROUP BY username, tweet;


Se crean los resultados en hdfs con avro streaming, de .avro a texto plano en formato .json:

hadoop jar hadoop-streaming-2.0.0-mr1-cdh4.3.0.jar 
                   -D mapred.job.name="avro-streaming"
                   -D mapred.reduce.tasks=0
                   -files avro-1.7.6.jar,avro-mapred-1.7.6-hadoop1.jar
                   -libjars avro-1.7.6.jar,avro-mapred-1.7.6-hadoop1.jar
                   -input  twitter/avro
               -output twitter/output/
               -mapper org.apache.hadoop.mapred.lib.IdentityMapper
               -inputformat org.apache.avro.mapred.AvroAsTextInputFormat

Pruebas:

- Si se borra el archivo .avro de hdfs y se inserta uno nuevo con el mismo nombre la tabla tweets se actualiza con los registros de este nuevo fichero.
- Si se a√±ade un nuevo archivo.avro en hdfs en la misma ruta la tabla tweets se actualiza con los registros de este nuevo fichero.
- El "create table tweets1 as..." vuelca los datos de la consulta a tweets1 sin necesidad de hacer ningun INSERT (aunque en formato texto).
- Cuando se lanza el mapreduce de creacion en hdfs de avro a json el/los fichero/s no se crean hasta que el job no termina por lo que no se pueden
  consultar hasta no finalizar la tarea. 