#Openbus

Big data applications for ingestion and analysis of massive amounts of events generated by a banking IT Infraestructure.

The objective of the postfix topology is to read the logs entries from kafka into a Trident Topology. Each register will be parsed and divided into fields.
The result will be stored in HDFS.

#Dependences

Deploying the Openbus architecture in your environment involves the following dependences:

- Hadoop 2.2.0 or higher version
- Storm 0.9.1 or higher version
- Kafka 0.8.8.1 or higher version
- Storm-HDFS plugin
- Storm-Kafka plugin

Since Storm-HDFS already has its own Hadoop dependencies (2.2.0 by default), if you are using a different version in your cluster, you must exclude the plugin dependency as follows:

```xml
<dependency>
  <groupId>com.github.ptgoetz</groupId>
  <artifactId>storm-hdfs</artifactId>
  <version>0.1.2</version>
  <exclusions>
		<exclusion>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-hdfs</artifactId>
		</exclusion>
		<exclusion>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-client</artifactId>
		</exclusion>
	</exclusions>
</dependency>
```

Once Storm-HDFS dependencies are excluded you must include you Hadoop version based dependencies.

#Running examples

Once the dependencies have been properly established, it is required to fill the properties file, in which all the cluster entries and outputs will be filled.
The required fields are:

- HDFS_URL: HDFS machine and port.
- HDFS_USER: User that will be used to write to HDFS.
- HDFS_OUTPUT_DIR: Output directory where the parsed data will be written.
- HDFS_OUTPUT_FILENAME: Prefix of the name of the output file.
- HDFS_OUTPUT_FILE_EXTENSION: Extension of the output file.
- INPUT_ORIGIN: Used for specifying where the data will be read from. Two values are allowed:
	- kafka: Data will be read from a kafka topic.
	- disco: Data will be read from a local file.
- INPUT_FILE: Input File url. Only used if "INPUT_ORIGIN=disco"

- KAFKA_ZOOKEEPER_LIST: List of Zookeeper for kafka <ip:port>,<ip:config>,...
- KAFAKA_BROKER_ID: Kafkas broker id
- KAFKA_TOPIC: Topic name of which data will be read from.
- KAFKA_FROM_BEGINNING: boolean value (true/false)


- STORM_CLUSTER: If the value is "local" the topology will be deployed in a LocalCluster.
- STORM_BATCH_MILLIS: Batch time width.
- STORM_NUM_WORKERS:Number of workers
- STORM_MAX_SPOUT_PENDING: Number of Max. spout Pending
- STORM_TOPOLOGY_NAME: Topology name

Some parameters for file rotation will be provided.
It is possible to rotate the file's name once it reaches a Max size or a certain amount of time has passed.

- SIZE_ROTATION_UNIT: Size unit. Allowed values KB/MB/GB/TB
- SIZE_ROTATION_VALUE: Size value.

- TIME_ROTATION_UNIT: Time unit. Allowed values SECOND/MINUTE/HOUR/DAY
- TIME_ROTATION_VALUE: Time value

- SYNC_MILLIS_PERIOD: Every "SYNC_MILLIS_PERIOD" millis the parsed data will be written to disk.

In this project 3 different topologies have been created:

- Postfix mailing
- Bluecuat proxy
- IronPort 

For each type of entry 2 classes will be created:

- Origin parser class: Trident BaseFunction which will allow us to process each register from the kafka topic and split it into fields. In this project `ProxyParser`, `PostfixParser` and `IronportParser`.
- Topology submiter: This class will submit each topology to the Storm cluster. In this project `OpenbusProxyTopology`, `OpenbusPostfixTopology` and `OpenbusIronportTopology`.

For running one of the topologies it is only needed to upload the JAR into Storm as follows:

``./bin/storm jar <JAR FILE> <Main Topology class> <properties file>`

For example:

`./bin/storm jar topologies-0.0.1-SNAPSHOT.jar com.produban.openbus.topologies.OpenbusProxyTopology proxy.properties`

